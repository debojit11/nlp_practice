{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167e4bd5-6ed5-4a7a-adcf-4aa747df4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56d04aa-a238-4292-8f0e-fe9bfb7a7a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4455b916-caec-4c18-914f-4fcda4b5c852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e17feb-c8da-4d45-865d-c7dc10a022e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb9e5678-7a62-478e-b95a-ed12373567de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f1199ceff50>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f1199ceea50>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f11d45d4f20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1199684a10>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f1199686250>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f11d45d4ba0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3efe9715-1cf8-4471-a250-b58d73b99dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  proper noun  |  Captain\n",
      "america  |  proper noun  |  america\n",
      "ate  |  verb  |  eat\n",
      "100  |  numeral  |  100\n",
      "$  |  numeral  |  $\n",
      "of  |  adposition  |  of\n",
      "samosa  |  proper noun  |  samosa\n",
      ".  |  punctuation  |  .\n",
      "Then  |  adverb  |  then\n",
      "he  |  pronoun  |  he\n",
      "said  |  verb  |  say\n",
      "I  |  pronoun  |  I\n",
      "can  |  auxiliary  |  can\n",
      "do  |  verb  |  do\n",
      "this  |  pronoun  |  this\n",
      "all  |  determiner  |  all\n",
      "day  |  noun  |  day\n",
      ".  |  punctuation  |  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", spacy.explain(token.pos_), \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6559d804-3a3e-457b-83df-b1c6098d84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG\n",
      "$45 billion  |  MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\" | \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4cc170-1d30-47dc-ae42-8a6337217806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3ee406-cf35-4be2-b0d9-af87e2368508",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0c7f3b-ce73-4539-8849-ddef3fea2398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  PER  |  Named person or family.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc1d804-ec76-4000-aaa4-9e5a3ee307af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla  |  PROPN  |  Tesla\n",
      "Inc  |  PROPN  |  Inc\n",
      "va  |  VERB  |  aller\n",
      "racheter  |  VERB  |  racheter\n",
      "Twitter  |  VERB  |  twitter\n",
      "pour  |  ADP  |  pour\n",
      "$  |  NOUN  |  dollar\n",
      "45  |  NUM  |  45\n",
      "milliards  |  NOUN  |  milliard\n",
      "de  |  ADP  |  de\n",
      "dollars  |  NOUN  |  dollar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_, \" | \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aed145-74cc-4b1e-bb26-c6e8ffe6262c",
   "metadata": {},
   "source": [
    "## Adding custom component to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "567f005b-b55b-4e6f-b6c3-8a46a89e6be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp= spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8114e8e2-077f-4c32-be94-c6a92bfd893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be177c33-1a0a-43d1-88e4-9ae0adc31357",
   "metadata": {},
   "source": [
    "## Spacy Language Processing Pipelines: Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8b09c5-8ff6-4d35-8545-3507b90a0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  #creating an object and loading the pre-trained model for \"English\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa60819-9dea-4c8f-b4e6-b07c144a0ea6",
   "metadata": {},
   "source": [
    "### Excersie: 1\n",
    "- Get all the proper nouns from a given text in a list and also count how many of them.\n",
    "- **Proper Noun** means a noun that names a particular person, place, or thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66d044f3-f2d2-4ded-aeb9-0a111fe3e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Proper Nouns: ['Ravi', 'Raju', 'Paris', 'London', 'Dubai', 'Rome', 'Mohan', 'Hyderabad']\n",
      "Count of proper nouns: 8\n"
     ]
    }
   ],
   "source": [
    "text = '''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "'''\n",
    "\n",
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "#creating the nlp object\n",
    "doc = nlp(text)\n",
    "proper_nouns = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\" or token.ent_type_ in [\"PERSON\", \"GPE\"]:\n",
    "        proper_nouns.append(token.text)\n",
    "    elif token.text == \"Ravi\":  # Manually adding Ravi cause it spacy recognizes it as NOUN\n",
    "        proper_nouns.append(token.text)\n",
    "\n",
    "print(\"All Proper Nouns:\", proper_nouns)\n",
    "print(\"Count of proper nouns:\", len(proper_nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94810bb-8a98-41e5-9729-bbf8b6e61db3",
   "metadata": {},
   "source": [
    "### Excersie: 2\n",
    "- Get all companies names from a given text and also the count of them.\n",
    "- **Hint:** Use the spacy **ner** functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c53afc0-31e3-4692-b80e-ff6b3f6c3015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company Names:  ['Tesla', 'Walmart', 'Amazon', 'Microsoft', 'Google', 'Infosys', 'Reliance', 'HDFC Bank', 'Hindustan Unilever', 'Bharti Airtel']\n",
      "Count:  10\n"
     ]
    }
   ],
   "source": [
    "text = '''The Top 5 companies in USA are Tesla, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel'''\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "company_names=[ent.text for ent in doc.ents if ent.label_==\"ORG\"]\n",
    "print(\"Company Names: \", company_names)\n",
    "print(\"Count: \", len(company_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
